[TOC]

# 分词工具

这里主要介绍中文中可以使用的分词工具。

1. jieba
2. thulac
3. snownlp
4. pynlpir
5. corenlp
6. pyltp
7. hanlp

## jieba

[github地址](https://github.com/fxsjy/jieba)

1. 特点：

- 支持四种分词模式：
  - 精确模式，试图将句子最精确地切开，适合文本分析；
  - 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；
  - 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
  - paddle模式，利用PaddlePaddle深度学习框架，训练序列标注（双向GRU）网络模型实现分词。同时支持词性标注。paddle模式使用需安装paddlepaddle-tiny，pip install paddlepaddle-tiny==1.6.1。目前paddle模式支持jieba v0.40及以上版本。jieba v0.40以下版本，请升级jieba，pip install jieba --upgrade 。PaddlePaddle官网

- 支持繁体分词
- 支持自定义词典

2. 算法

- 基于前缀词典实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图 (DAG)
- 采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合
- 对于未登录词，采用了基于汉字成词能力的 HMM 模型，使用了 Viterbi 算法

## snownlp

[github地址](https://github.com/isnowfy/snownlp)

代码已经很久了，最近的更新已是三年前。

包含以下功能：

- 情感分析（现在训练数据主要是买卖东西时的评价，所以对其他的一些可能效果不是很好，待解决）

- 文本分类（Naive Bayes）
- 转换成拼音（Trie树实现的最大匹配）
- 繁体转简体（Trie树实现的最大匹配）
- 提取文本关键词（TextRank算法）
- 提取文本摘要（TextRank算法）
- Tokenization（分割成句子）
- 文本相似（BM25）

## thulac

出自于清华大学自然语言处理与社会人文计算实验室。

[github地址](https://github.com/thunlp/THULAC-Python)

THULAC具有如下几个特点：

- 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。
- 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。
- 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。

## PyNLPIR

大数据搜索挖掘实验室（北京市海量语言信息处理与云计算应用工程技术研究中心）推出的。

[github地址](https://github.com/tsroten/pynlpir)

A Python wrapper around the NLPIR/ICTCLAS Chinese segmentation software。

注意：这个只是个客户端，用户访问服务端才能提供相应的功能。

## CoreNLP

是由斯坦福的相应实验室开发，集成了很多的NLP功能。

[官方网页](https://stanfordnlp.github.io/CoreNLP/)

## HanLP

[github地址](https://github.com/hankcs/HanLP)

注意：之前版本的HanLP是Java版本的，但是最新的是Python语言写的。

## 参考

1. [六款中文分词模块尝试:jieba、THULAC、SnowNLP、pynlpir、CoreNLP、pyLTP](https://cloud.tencent.com/developer/article/1010981)
