[TOC]

# 文本匹配简介

文本匹配是自然语言处理中的一个很基础的问题，NLP中很多的任务其实都是一个文本匹配的任务。比如：搜索中匹配网页内容和用户搜索词。自动问答中问句匹配候选答案。还有，文本的去重也可以抽象为文本相似度匹配的问题。

## 1. 传统文本匹配技术及问题

传统的文本匹配技术如信息检索中的向量空间模型VSM、BM25等算法，这些算法解决的是词汇层面的相似度匹配问题，但是纯粹的基于词汇的相似度匹配是有很大的局限的，原因如下：

1. 语言的一词多义和一义多词
   相同的词在不同的语境下表示不同的含义，比如：苹果，既可以表示水果，又可以表示科技公司。同样，相同的语义可以用不同的词表示，甚至是用不同的句子表示。

2. 语言的组合结构问题
   相同的词组成的句子，不同的语序就可以表示完全不同的含义。比如：A打败B，B打败A。

3. 匹配的非对称问题
   文本匹配类的任务，有些时候并不简单是文本的相似问题。一方面，不一定要求语言上的类似，比如搜索任务中的搜索语句和网页端往往具有很大的差别，至少在长度上就差别很大。另一方面，也不一定要求语义上的相似，比如问答系统中问题和答案可能并不是语义上相似，但是能回答所问的问题。

基于上面的问题，我们可以得出结论，文本匹配任务，不能单纯停留在字面的匹配上，更需要语义的匹配。同时，也不仅仅是语义相似的匹配，而是更广意义上的匹配。

## 2. 基于潜在语义分析的文本匹配

上世纪90年代流行起来的潜在语义分析技术(Latent Semantic Analysis, LSA)，开辟了一个新思路，将词句映射到等长的低维连续空间，可在此隐式的空间上进行相似度计算。

此后，又有PLSA、LDA等更为高级的概率模型被设计出来，逐渐形成了非常火热的主题模型技术方向。

这些技术对文本的语义表示形式简洁、运算方便，较好的弥补了传统词汇匹配方法的不足。不过从效果上看，这些技术都无法替代字面匹配技术，只能作为字面匹配技术的弥补。

## 3. 基于深度学习的文本匹配

深度学习兴起后，基于神经网络训练出的Word Embedding来进行文本的匹配计算引起了大家广泛的兴趣。Word Embedding的训练方式更为简洁，而且所得的词语向量表示的语义可计算性进一步增强。

但是，利用无标注数据训练的到的Word Embedding在匹配计算上和主题模型计算的效果和主题模型没有大的区别，因为它们本质上都是基于共现信息的训练。另外，Word Embedding并没有解决短语、句子的语义表示问题，也没有解决匹配的非对称性问题。

为了克服以上的不足，百度于2013年设计研发了一种有监督的基于神经网络的语义匹配模型SimNet，大幅提升了匹配计算的效果。（注：[有关SimNet会有专门的介绍](./SimNet.md)）

除了百度的SimNet，相关的研究还有：Microsoft Research提出的DSSM模型(Deep Structured Semantic Model)；华为NOAH'S ARK LAB提出的新的神经网络匹配模型的变体，如基于二维交互匹配的卷积匹配模型；中科院等研究机构提出的诸如多视角循环神经网络匹配模型(MV-LSTM)、基于矩阵匹配的层次化匹配模型MatchPyramid等更加精致的神经网络文本匹配模型。
