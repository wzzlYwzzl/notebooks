[TOC]

# 使用Sklearn的Gradient Boosting时调参

Sklearn中与梯度提升有关的两个类分别是：GradientBoostingRegressor和GradientBoostingClassifier，它们分别用于处理回归问题和分类问题。

两个类的参数基本是相同的，loss函数有少许差异。参数可以分为两类：

1. Boosting框架的参数；
2. 弱学习器的参数，即决策树的参数；

## 1. Boosting框架的参数

### 1.1 n_estimators

弱学习器的最大迭代次数，也就是最大的弱学习器个数。默认100。

### 1.2 learning_rate

每个弱学习器的权重缩减系数。

$$f_k(x) = f_{k-1}(x) + v*h_k(x)$$

这里的v就是learning_rate，这是一种正则化方式，加载模型输出上其实类似于L1和L2正则项，减小了模型参数，较小的v意味着更多的弱学习器才能达到效果，所以这个参数和n_estimator要一起调节。

### 1.3 subsample

子采样率，$(0,1]$。这和随机森林的有放回采样不一样，这里是无放回的。就是用采样得到的数据去拟合决策树。

这也是正则化的一种方式。

**小思考**：正则化的目标就是降低模型的复杂度，让模型的表达能力不要太强导致过拟合。子采样让模型需要拟合的样本变少了，那么子模型的复杂度会降低。

### 1.4 init

初始的弱学习器，也就是$f_0(x)$。

### 1.5 loss

使用的损失函数。

**对于分类模型**：

- “deviance”是对数似然损失函数；
- “exponential”是指数损失函数。

一般使用deviance，它对分类问题做了优化，而exponential损失相当于把模型变为了Adaboost算法。

**对于回归模型**：

- “ls”：均方差
- “lad”：绝对损失
- “huber”：Huber损失
- “quantile”：分位数损失

如果数据的噪音点不多，用默认的均方差。如果噪音比较多，推荐使用抗噪音的损失huber。如果我们对训练集进行分段预测的时候，则采用“quantile”。

### 1.6 alpha

这个是GradientBoostingRegressor有的参数。当我们使用“huber”或“quantile”时，需要指定分位数的值。默认0.9，如果噪音点较多，可以适当降低这个分位数的值。

## 2. 弱学习器参数

GBDT使用了CART回归决策树，因此它的参数基本源于决策树类，即DecisionTreeClassifier和DecisionTreeRegressor。

### 2.1 max_features

划分时考虑的最大特征数。

- 默认None，表示考虑所有特征；
- “log2”表示最多考虑$log_2{N}$个特征；
- “sqrt”或“auto”意味着最多考虑$\sqrt{N}$个参数；
- 整数。指定使用特征的具体数量。
- 浮点数。使用特征的百分比。

如果特征数量不大，比如小于50，那么可以使用默认None，如果特征数量比较大，可以限制使用特征的数量来加快决策树的生成。

### 2.2 max_depth

决策树的最大深度。

如果特征和样本数据不多的情况，那么不需要限制。常用的取值10~100。

### 2.3 min_samples_split

内部节点划分时所需要的最小样本数。这个可以用于限制子树的划分，如果样本小于这个数量，那么就不会再进行划分。

### 2.4 min_samples_leaf

限制叶子节点最少的样本数。如果叶子节点的样本数小于这个值，则会把这个节点及兄弟节点一起剪掉。

### 2.5 min_weight_fraction_leaf

叶子节点最小的样本权重。如果叶子节点的所有样本权重和的小于这个值，那么会和兄弟节点一起剪枝。

默认是0，表示不考虑权重问题，但是如果较多的样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时就要注意这个值了。

### 2.6 max_leaf_nodes

最大叶子节点数，为了防止过拟合。如果加了限制，那么算法会在这个限制下建立最优的决策树。

### 2.7 min_impurity_split

节点划分最小不纯度。如果某个节点的不纯度(基于基尼系数，均方差)小于这个阈值，那么该节点不再产生子节点。一般不推荐改动这个默认值1e-7。
