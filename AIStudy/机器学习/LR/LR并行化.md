z[TOC]

# Logistic Regression并行化

## 1. 参考资料

1. [LR并行化实现](https://blog.csdn.net/qq_32742009/article/details/81839071)

## 2. 并行化原理分析

LR学习w参数时的梯度公式如下：
设learning_rate为$\alpha$，那么w的更新公式如下：
$\begin{aligned}
w_{j+1}
&= w_j + \alpha\frac{\partial L}{\partial w} \\
&=w_j + \alpha \sum(y_ix_i-x_i\pi(x_i)) \\
&=w_j + \alpha \sum x_i(y_i-\pi(x_i))
\end{aligned}$

并行化的核心就是对w的更新过程进行并行化，由于上面的计算过程就是矩阵的乘法和矩阵的加法运算，并行化就是对矩阵运算过程的并行化，实质就是对矩阵进行分块，然后分块计算，最后再进行合并。

### 2.1 输入数据向量化

假设模型训练的样本有M个，每个样本$X_i$有N个特征，也即是个N维向量。这样，训练样本组成一个$M*N$的矩阵。标签构成一个$M*1$的向量。
![LR Parallel](./images/LR-Parallel-1.jpg)

参数w是一个$1*N$，这样$w$和$x$是$wx$的关系。

### 2.2 并行化过程分析

#### 2.2.1 数据分割

将$M*N$的数据矩阵分割为$m*n$，也就是要将这些数据提交个$m*n$个节点进行并行计算。所以要将这些数据分割为$m*n$块。

![LR-parallel-2](./images/LR-parallel-2.jpg)

一个样本的特征向量被分拆为n个子向量。
$$X_{r,k} = <X_{(r,1),k}, X_{(r,2),k}, ...,X_{(r,n),k}>$$

同样，参数w也被分割为n个子向量。
$$W = <W_1,...,W_c,...,W_n>$$

### 2.3 总结

看上面的公式什么的可能很复杂，符号标记既模糊又复杂。概括一下，其实很简单。

1、按行并行。
这中做法拆解的是w更新公式中的$\sum$。这种做法解决的是如果参数的纬度并不是很大，但是训练样本很多，那么这种并行就可以解决这个问题。

即将样本拆分到不同的机器上去。其中。比如我们按行将其均分到两台机器上去，则分布式的计算梯度，只不过是每台机器都计算出各自的梯度，然后归并求和再求其平均。为什么可以这么做呢？因为公式只与上一个时刻的以及当前样本有关。所以就可以并行计算了。

2、按列并行。
这种做法是拆解的参数的纬度。说白了就是将高维的参数拆分成多个低维的子变量，然后在做矩阵运算时进行分块运算，需要合并时，同常涉及矩阵的乘法时，是需要合并的，就把多个计算节点的结果在主节点进行合并。这种做法可以应对有些问题中特征的纬度高达上亿甚至更多的情形。

按列并行的意思就是将同一样本的特征也分布到不同的机器中去。上面的公式为针对整个，如果我们只是针对某个分量，可得到对应的梯度计算公式即不再是乘以整个，而是乘以对应的分量，此时可以发现，梯度计算公式仅与中的特征有关系，我们就可以将特征分布到不同的计算上，分别计算对应的梯度，最后归并为整体的，再按行归并到整体的梯度更新。
