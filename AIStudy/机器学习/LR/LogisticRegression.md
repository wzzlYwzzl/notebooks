[TOC]

# Logistic Regression

## 1. 参考

1. 《统计学习方法》李航
2. [logistic回归原理解析及Python应用实例](https://blog.csdn.net/feilong_csdn/article/details/64128443)
3. [Logistic Regression的前世今生](https://blog.csdn.net/cyh_24/article/details/50359055)

## 2. LR基础知识

### 2.1 模型的定义

LR模型的形式是如下的条件概率分布：
$P(Y=1|x) = \dfrac{e^{wx}}{1+e^{wx}} = \pi(w,x)$
$P(Y=0|x) = \dfrac{1}{1+e^{wx}} = 1-\pi(w,x)$

利用LR模型可以处理二分类问题。

上面的模型可以合并为一个公式：
$P(y|x) = (\pi(w,x))^y*(1-\pi(w,x))^{1-y}$

### 2.2 LR模型参数求解

在机器学习中，我们通常是通过定义损失函数，并最小化损失函数来学习模型参数的。常用的损失函数包括0-1损失、平方损失、绝对损失函数、对数损失函数。

但是，LR模型使用的是极大似然估计法来学习参数。为什么不用损失函数呢？这个问题留到后面去解答。

似然函数如下：
$\prod\pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i}$

对数似然函数为：
$L(w) = \sum\limits_i^n[y_iln\pi(x_i) + (1-y_i)ln(1-\pi(x_i))]$

参数学习方法就是通过对上面的对数似然函数利用梯度上升或者拟牛顿法学习参数w。

对数似然对w求导得到：
$\frac{\partial L}{\partial w} = \sum (y_ix_i-x_i\pi(x_i))$

梯度上升法求解w：
设learning_rate为$\alpha$，那么w的更新公式如下：
$\begin{aligned}
w_{j+1}
&= w_j + \alpha\frac{\partial L}{\partial w} \\
&=w_j + \alpha \sum(y_ix_i-x_i\pi(x_i)) \\
&=w_j + \alpha \sum x_i(y_i-\pi(x_i))
\end{aligned}$

## 3. LR相关问题

### 3.1 为什么LR能比线性回归好？

先说说二者之间的联系和区别：

1. 二者都是广义线性回归；
2. 线性回归优化的目标函数是平方损失，logistic回归则是似然函数；
3. 线性回归在整个实数域范围内进行预测，敏感度一致；而分类范围，需要在[0,1]。Logistic回归减小预测范围，将预测值限定为(0,1)之间的一种回归模型，因为对于这种问题，Logistic的鲁棒性要好于线性回归。

为什么LR比传统线性回归好？上面区别中的第三条说明了这个原因。这里再简单总结一下：

线性回归预测的范围是整个实数域，而logistic的预测范围则是在[0,1]区间。线性回归在整个实数域范围内敏感度一样，但是Logistic在0附近敏感度高，在离0比较远的位置敏感度则很低。

### 3.2 LR和最大熵的关系？

关于这个问题的答案，等后面整理完了最大熵模型之后再来回答。

### 3.3 并行化的LR是怎么做的？

LR并行化有专门的内容来介绍。

### 3.4 LR使用极大似然估计学习参数，为什么不用最小化损失函数的方式呢？

因为极大似然函数等价于最小化对数损失函数。

证明过程如下：
对数损失函数的公式为：$Loss(Y,P(Y|X)) = -log(P(Y|X))$，根据这个定义，加上前面的公式：$P(y|x) = (\pi(w,x))^y*(1-\pi(w,x))^{1-y}$，我们可以得到损失函数如下：
$Loss = -log[(\pi(w,x))^y*(1-\pi(w,x))^{1-y}]$

如果我们对整个数据集上计算损失函数，那么我们就发现其实二者是等价的。
