[TOC]

# 最大熵

## 1. 有关熵的基础知识

### 1.1 什么是熵？

度量不确定性的一种方式，越不确定熵越大。这里有一个容易混淆的问题：

离散型随机变量X的熵：

$$H(P) = -\sum_x {P(x)logP(x)}$$

我们经常用概率来衡量不确定性，比如硬币正面的概率为0.5，一个骰子得到某一面的概率为1/6。但是，注意这里的概率衡量的不是随机变量本身，而是随机变量取某个值的可能性。通过概率我们无法量化比如硬币抛币这个整体的的不确定性。而这个是要通过熵来衡量的。

### 1.2 联合熵、条件熵、相对熵、互信息

联合熵：$H(X,Y) = -\sum_{x,y} P(x,y)logP(x,y)$

条件熵：$H(X|Y) = H(X,Y) - H(Y) = -\sum_{x,y}p(x,y)logp(x|y)$

相对熵：$D(p||q) = \sum_x p(x)log\dfrac{p(x)}{q(x)}$

几种熵的关系可以用下图展示：左边椭圆代表H(X)，右边椭圆代表H(Y)。

![entropy](./images/entropy.png)

## 2. 最大熵模型的定义


