[TOC]

# 交叉熵

交叉熵是信息论中的概念，用来评估平均编码长度。

给定两个概率分布p和q，通过q表示p的交叉熵为：$$H(p,q) = -\sum_x{p(x)logq(x)}$$

**思考**：交叉熵是用来衡量两个概率分布之间的关系的，也就是说一个随机变量的真实分布为p，但是我现在用概率分布q来作为随机变量的分布的话，这带来的差异性我们可以用交叉熵来表示。如果交叉熵越小，那么这两个分布越接近。

交叉熵衡量的是两个分布之间的共性程度。相对熵衡量的是两个分布之间的差异程度。

## 1. 模型输出概率化

我们在处理多分类任务时，最后一层同常都会经过Softmax。它的作用就是把输出转化成概率分布。
$$softmax(y)_i = \frac{e^{y_i}}{\sum^n_{j=1}e^{y_i}}$$

**思考**：看起来softmax就是做了一件事：把每个变量取值转为正，然后再进行归一化。

## 2. Example

假设有一个三分类任务，某个样例的正确输出是(1,0,0)，这个模型的softmax输出是(0.5,0.4,0.1)，那么预测和正确答案之间的交叉熵是：
$H((1,0,0),(0.5,0.4,0.1))$
$=-(1*log(0.5) + 0*log(0.4) + 0*log(0.3))$
$\approx 0.3$

## 3. 为什么采用交叉熵损失函数

## 4. tensorflow交叉熵函数

```python
cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(label=y_, logits=y)

"""
y表示原始输出，还没有经过softmax的输出，y_是给出的标准答案。
"""
```
