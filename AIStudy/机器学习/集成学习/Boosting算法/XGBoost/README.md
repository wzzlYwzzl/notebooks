[TOC]

# XGBoost

XGBoost，Extreme Gradient Boosting，经常用在比赛中，效果显著。XGBoost是GBDT的改进，可以用于分类问题，也可以用于回归问题。

## 1. 基本概念

### 1.1 回归树与决策树

决策树用于分类问题，回归树当然就是用于回归问题。分类树输出的是类别，回归树输出的是数值。

回归问题我们无法使用信息增益、信息增益率、基尼系数来判定树如何分类最优了，这个时候，我们会使用回归问题中常用的均方误差、对数误差等。

### 1.2 boosting集成学习

集成学习的核心思想就是用多个模型联合解决学习问题，以使模型有更强的泛化能力。不同的集成算法结合不同模型的思想是不一样的。常见的集成思想有：bagging、boosting、stacking。

bagging集成学习的代表是随机森林。各个决策树是独立的，每个决策树通过在样本集中随机选择一批样本，独立训练，然后将多个决策树合在一起进行投票来作为最终结果。

boosting集成学习的多个模型之间是有串联关系的，这种串联关系的体现就是加法模型。新的子模型就是用来拟合前面模型的残差。举个例子：一个样本：$x_1=(2,4,5), y_1 = 4$,第一个决策树训练之后，对于这个样本的预测值是3.3，那么第二棵决策树的训练样本就变成了$x_1=(2,4,5),y_1 = 0.7$。

XGBoost使用的CART树(分类回归树)作为基础模型来进行提升学习的。

## 2. XGBoost求解思路

我们分析如何优化一个机器学习模型，我们都要先确定目标函数，目标函数中包含了模型的参数，通过最小化目标函数，达到学习模型参数的效果。

目标函数的一般形式：$L = \sum_{i=1}^n{l(y_i,\hat{y}_i)} + \Omega(f)$。

对于树模型，$\Omega(f) = \gamma T + \frac{1}{2}\lambda\sum_{j=1}^T{w_j}^2$，其中$T$表示叶子节点的数量，$w_j$表示叶子节点的得分，也就是分到该叶子节点的样本的预测值。

在求解树模型时，涉及两个基本问题：

- 1.每次分裂时，选择哪个特征？
- 2.如何确定叶节点的预测值w。

分裂特征的选择方法：
由于树模型的结构是不能通过简单的数学公式优化来求解最优结构的，求解树结构的方法就是贪心算法。每一次选择feature进行节点分裂时，分别计算选用不同feature时的损失函数，选择损失函数最小的作为分裂特征。

每个节点预测值w的确定：
它的确定是通过优化算法求得的，不是简单的取平均值。在推导过程中会展示如何求解w。

## 3. XGBoost推导过程

Boosting算法的模型都是如下函数：$$\hat{y_i} = \sum_{k=1}^{K}f_k(x_i)$$，这是一个加法模型。

加法模型的求解思路就是如下的一个递归过程：

1. 初始化：$\hat{y_i}^{(0)} = 0$，也就是模型对任何输入的预测都是0。
2. 模型中添加第一个子模型，比如一棵树模型：$\hat{y}_i^{(1)} = \hat{y}_i^{(0)} + f_1(x_i)$，其中$f_1(x_i)$就是我们这里要添加的模型。这个模型的样本会变成$(x_i,y_i-\hat{y}_i^{(0)})$。
3. 以此类推，每一步求解一个新的模型。

XGBoost目标函数的变换推导过程：
>$Obj(\theta) = \sum_{i=1}^n{l(y_i,\hat{y}_i^{t})} + \sum_{k=1}^t{\Omega(f_k)}$

因为在求解第$t$个模型时，前面$t-1$个模型的复杂度就是常数了，所以：
>$Obj=\sum_{i=1}^n{l(y_i,\hat{y}_i^{t-1} + f_t(x_i))} + \Omega(f_t) + C$

这里假设损失函数$l(y_i, \hat{y}_i)$是一般损失函数，且对$\hat{y}_i$有一阶导和二阶导，那么可以做泰勒二阶展开。ps. 泰勒公式如下：$f(x+\Delta x) \approx f(x) + f^{'}(x)\Delta x + \frac{1}{2}f^{''}(x)\Delta x^2$。令$f_t(x_i) = \Delta x$，目标函数变成：
>$Obj=\sum_{i=1}^n[l(y_i,\hat{y}_i^{t-1}) + \partial_{\hat{y}^{t-1}}l(y_i,\hat{y}_i^{t-1})f_t(x_i) + {\frac{1}{2}}\partial^2_{\hat{y}^{t-1}}l(y_i,\hat{y}_i^{t-1})f_t^2(x_i)] + \gamma T + \frac{1}{2}\lambda\sum_{j=1}^T{w_j}^2 + C$

令：$g_i = \partial_{\hat{y}^{t-1}}l(y_i,\hat{y}_i^{t-1})$，$h_i = \partial^2_{\hat{y}^{t-1}}l(y_i,\hat{y}_i^{t-1})$，则上面的目标函数可以化简为：
>$Obj=\sum_{i=1}^n[g_if_t(x_i) + {\frac{1}{2}}h_i f_t^2(x_i)] + \gamma T + \frac{1}{2}\lambda\sum_{j=1}^T{w_j}^2 + C$

一个树模型可以表示成如下一个函数：$f_t(x) = w_{q(x)}, w \in R^T, q: R^d \rightarrow [1,...,T]$, w表示叶子节点的得分值，q(x)表示样本x对应的叶子节点，T是叶子节点的数量。现定义$I_j={i|q(x_i)=j}$，分到表示第j个叶子节点上的样本。下面可以对上面的目标函数再做如下的化简：
>$Obj=\sum_{j=1}^T[\sum_{i \in I_j}g_iw_j + {\frac{1}{2}}\sum_{i \in I_j}h_iw_j^2] + \gamma T + \frac{1}{2}\lambda\sum_{j=1}^T{w_j}^2 + C$
>$=\sum_{j=1}^T[\sum_{i \in I_j}g_iw_j + {\frac{1}{2}}\sum_{i \in I_j}(h_i + \lambda) w_j^2] + \gamma T + C$

令$G_j = \sum_{i \in I_j}g_i, H_j=\sum_{i \in I_j}h_i$，则目标函数可再次化简：
>$Obj=\sum_{j=1}^T[G_jw_j + \frac{1}{2}(H_j + \lambda)w_j^2] + \gamma T + C$

上面目标函数对$w_j$求导，可以得到：
>$w_j^* = - \frac{G_j}{H_j + \lambda}$
>$Obj = - \frac{1}{2}\sum_{j=1}^T{\frac{G_j^2}{H_j + \lambda}} + \gamma T$

经过上面的推导，我们知道了：当树的结构确定时，每个叶子节点的得分w和当前树结构下的损失大小。这为我们构建树提供了指导。我们构建新的分支时，根据Obj损失减少的最大的特征作为新的分支依据。同时我们也能知道新的分支的叶子节点上的得分w。

## 4. XGBoost的主要特点

- 每个叶子节点的w值是求出来的，不是求平均值或者规则指定。
- 正则化防止过拟合。
- 支持自定义损失函数。
- 支持并行化：选择最佳分裂点时，需要进行枚举，以找到最好的分割点，这是可以进行并行化的地方，也是损耗性能的地方。
- 特别设计了针对稀疏数据的算法。
- 可实现后剪枝。
- 交叉验证。
- 行采样、列采样，随机森林的套路。
- Shrinkage：模型伸缩，优点类似于深度学习中的learning rate，每个叠加的模型都学习一定比率的预测值。比如：label是4.0，第一个模型的预测值是3.3，那么可以对这个预测值打个折，比如取0.3，那么第二个模型学习的残差就是：4.0 - 0.3 * 3.3 = 3.01。这可以防止过拟合。
- xgboost还支持设置样本权重，这个权重体现在梯度g和二阶梯度h上，优点类似于adaboost。

## 5. 与GBDT对比

Xgboost第一感觉就是防止过拟合+各种支持分布式/并行，所以一般传言这种大杀器效果好（集成学习的高配）+训练效率高（分布式）。

## 6. 与深度学习对比

Xgboost和深度学习的关系，陈天奇在Quora上的解答如下：

不同的机器学习模型适用于不同类型的任务。深度神经网络通过对时空位置建模，能够很好地捕获图像、语音、文本等高维数据。而基于树模型的XGBoost则能很好地处理表格数据，同时还拥有一些深度神经网络所没有的特性（如：模型的可解释性、输入数据的不变性、更易于调参等）。

## 参考地址

1. [中文文档](https://xgboost.apachecn.org/#/)
2. [Boosting家族只XGBoost](https://www.cnblogs.com/zongfa/p/9324684.html)
3. [深入理解XGBoost](https://zhuanlan.zhihu.com/p/83901304)
