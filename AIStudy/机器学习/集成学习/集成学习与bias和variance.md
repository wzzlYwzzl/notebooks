[TOC]

# 集成学习中的方差和偏差问题

方差和偏差是评价模型好坏的两个重要维度。

**偏差**是说模型的预测值与真实值之间的偏差，反映模型的拟合能力。

**方差**度量了同等大小规模的训练数据集发生变化时，模型的学习性能的变化程度，反映模型的抗干扰能力。

**模型越复杂**，拟合能力越好，偏差越小。但是换一组数据时，模型的变化就会很大，所以复杂的模型容易过拟合，故方差偏大。**模型简单**时，更换数据，对模型的输出影响不会很大，方差偏小，但是由于模型简单，偏差就会比较大。

## 1. Bagging和Boosting与方差、偏差

Bagging是通过并行训练独立的多个基础模型，然后多个模型通过投票或者取平均得到结果，Bagging可以通过多个模型来**降低整体的方差**，因为多个模型是不一样的，通过voting可以降低数据波动对模型的整体效果的影响。每个基模型的目标就是要降低偏差，所以我们采用深度很深且不剪枝的决策树。

Boosting集成方法中，每一轮模型的目标是在上一轮模型的基础上更加拟合原数据，这可以保证模型的准确性，**降低偏差**，但是模型的方差就会偏大，所以选择基模型时，优先选择方差更小的分类器，即使用简单的弱分类器，所以我们选择深度很浅的决策树。
