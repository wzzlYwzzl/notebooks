[TOC]

# Multi-Task DNN for Natural Language Understanding

## Abstract

MT-DNN借助多个NLU任务来学习表示。MT-DNN得益于两部分内容：大量的cross-task数据和regularization effect。

MT-DNN通过把预训练Bert模型应用于《Representation learning using multi-task deep neural networks for semantic classiﬁcation and information retrieval.》这篇论文提到的MT-DNN中(MT-DNN在这篇论文中就出现了)。

MT-DNN在10个NLU任务上都有达到了SOTA，使GLUE benchmark提升了2.2个百分点。

MT-DNN代码：https://github.com/namisan/mt-dnn.

## Introduction

学习文本的向量表示是很多NLU任务的基础，有两种常用的方式：multi-task learning(MTL)和language model pre-training。

本论文提出的MT-DNN模型就是结合了这两种方式。

**MTL**受人的学习行为的启发。比如会滑雪板的人学习溜冰鞋比其他人不会的人会更容易。所以，多任务联合学习有助于在一个任务上学习到的知识用于另一个任务。现在大家多将深度网络表示应用于MTL表现出了越来越多的兴趣。因为监督学习需要很多的语料，MTL一个优势就是可以同时使用多个任务的语料来训练表示，不仅增加了学习的语料，同时也防止了DNN representation过拟合于单个任务。

**思考**：如果不同的任务所需要的基础能力是一致的，或者相似的，那么这些任务就可以考虑联合学习。

与MTL不同的是，language model pre-training展示出通过大量无标注数据学习统一的表示的有效性。比较有名的有：ELMo、GPT、Bert。这些模型都是在大量文本数据上使用了无监督目标。比如bert基于多层的双向transformer，通过预测被masked的单词和预测下一个句子来进行训练。这些模型要用于具体的任务时，需要进行fine-tuning。
