[TOC]

# Word2Vec

NLP对应的问题，处理的基本素材就是词、句、段、篇章。词是语义的最小单元，也是NLP任务所要面对和处理的最基本对象。

要用机器学习算法中应用这些NLP素材，必须要对这些素材进行数字化表示。词嵌入便是一种比较好的词表示方式，而word2vec便是Google提出的一种比较有效的词嵌入方式。

## 1. 从一个举例开始

我们先从一个简单的例子来理解word2vec是如何学习词向量的，或者说如何学习词与词之间的关系的。

1. “她们 夸 吴彦祖 帅 到 没朋友”，这句话中“她们”、“夸”、“吴彦祖”、“帅”、“到”、“没朋友”因为在一句话中一起出现了，那么这几个词的关系就比较近，而且是离得越近的两个词关系越近。
2. “她们 夸 我 帅 到 没朋友”，这句话中“我”和上一句中“吴彦祖”是同一个上下文，所以word2vec会认为“我” = “吴彦祖”。

上面的例子从两个维度上理解了word2vec的学习词向量的基本思路。

## 2. Skip-gram和CBOW

要获取词的向量表示，首先要明确的问题是：什么样的词向量表示才是好的表示？要通过学习的方式获取词向量，那么就要明确学习的目标，然后才能通过反向传播的方式来学习。word2vec通过定义了两个模型来定义何种表示才是好：skip-gram和CBOW。

**skip-gram**：能够用当前词的表示预测上下文词，那么这个表示就是好的。
**CBOW**：能够通过上下文的词表示来预测当前词，那么这个表示就是好的。

## 参考

1. [word2vec原理](https://www.cnblogs.com/pinard/p/7160330.html)
2. [理解word2vec之skip-gram](https://zhuanlan.zhihu.com/p/27234078)
3. [word2vec中的数学原理详解](https://blog.csdn.net/itplus/article/details/37969817)
4. [自然语言处理之词向量Word2vec详解](https://www.aboutyun.com/forum.php?mod=viewthread&tid=24390)
