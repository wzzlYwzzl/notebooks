[TOC]

# Keras Functional API

Keras的函数式接口是用于定义复杂的模型的，比如多输出模型，有向无环图，有共享层的模型等等。

## 第一个例子：一个全连接层连接的网络

这个例子使用Sequential更合适，但是这里为了演示functional API，所以没有使用它。

- layer实例是callable，传入一个tensor，输出一个tensor。
- 输入的tensor和输出的tensor可以用来定义一个model。

```python
from keras.layers import Input, Dense
from keras.models import Model

# 这个输出一个tensor
inputs = Input(shape=(784,))

output_1 = Dense(64, activation='relu')(inputs)
output_2 = Dense(64, activation='relu')(output1)
predications = Dense(10, activation='softmax')(output_2)

model = Model(inputs=inputs, outputs=predictions)
model.compile(optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.fit(data, labels)

```

## model也是callable，和layers一样

通过functional API，可以很容易地复用训练过的模型。可以把model视作一个layer，输入是tensor，输出是tensor。调用一个model时，不仅使用了这个模型的结构，同时也使用了这个模型的weights。

## 多输入和多输出模型

functional API的一个很好的用例：多输入、多输出模型。Keras的functional API可以很容易的创建大规模的相互交织的数据流

考虑下面一个问题：根据新闻标题预测会有多少转推和点击“喜欢”。

这个问题的模型对应的主要输入就是新闻标题，除此之外，还有辅助的输入数据，比如新闻标题产生的时间等等。这个模型通过两个损失函数来监督模型，对于深度模型，在模型早期使用损失函数是一种很好的正则化方式。

模型的结构图：![3](./images/3.png)。

下面是实现代码：

```python
from keras.layers import Input, Embedding, LSTM, Dense
from keras.models import Model
import numpy as np
# 设置随机数种子是为了方便重现
np.random.seed(0)

"""
输入是100个字长的header，每个字用一个整数表示，1~10000，表示它是第几个字。并指定这一层的名字是‘main_input’
"""
main_input = Input(shape=(100,), dtype='int32', name='main_input')

"""
嵌入层，每个字用512维表示，总共10000个字，输入的长度是100。
"""
x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)

"""
LSTM将向量的序列转换为一个向量。
"""
lstm_out = LSTM(32)(x)

"""
接下来插入辅助损失，使LSTM和Embedding层能够更顺利地训练，纵使模型的主损失依旧很高。
"""
auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)

"""
为模型添加辅助输入
"""
auxiliary_input = Input(shape=(5,), name='aux_input')
x = keras.layers.concatenate([lstm_out, auxiliary_input])

x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)
x = Dense(64, activation='relu')(x)

main_output = Dense(1, activation='sigmoid', name='main_output')(x)

"""
描述完网络结构，接下来定义模型
"""
model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])

"""
由于有两个输出，所以指定损失时可以为不同的损失函数指定不同的权重，以及指定不同的损失函数。有两种做法：通过一个list，或者通过一个dict。

方法一：通过一个list
由于传入一个loss，所以两个两个输出使用同一个损失函数。
"""
model.compile(optimizer='rmsprop', loss='binary_crossentropy', loss_weights=[1., 0.2])

"""
接下来就是训练模型：通过传入一个input的list和target的list。
"""
headline_data = np.round(np.abs(np.random.rand(12,100) * 100))
additional_data = np.random.randn(12,5)
headline_labels = np.random.randn(12,1)
additional_labels = np.random.randn(12,1)

model.fit([headline_data, additional_data],[headline_labels, additional_labels], epochs=50, batch_size=32)

"""
由于输入和输出都是有name的，所以可以通过dict的方式为模型指定训练参数和数据。
"""
model.compile(optimizer='rmsprop', loss={'main_output':'binary_crossentropy', 'aux_output':'binary_crossentropy'}, loss_weights={
    'main_output':1., 'aux_output':0.2
})

model.fit({'main_input':headline_data, 'aux_input':additional_data}, {'main_output':headline_labels, 'aux_output':additional_labels}, epochs=50, batch_size=32)

"""
利用训练好的模型做推理
"""
model.predict({'main_input':headline_data, 'aux_input':additional_data})

# 或者如下方式
model.predict([headline_data, additional_data])
```

## 共享层(shared layers)

另一个使用场景就是“shared layers”，也就是模型共用layer。

考虑如下一个问题，我们希望构建一个模型来告诉我们两个tweets是否是来自于一个人。

一种做法就是分别对两个tweets进行编码，得到两个向量，然后将两个向量拼接成一个向量，然后经过一个logistic回归，得到一个概率来反映这两个向量是否是相关的。

由于对两个tweets编码真实同一个问题，所以编码层应该被复用。下面我们就用一个共享的LSTM层来编码两个tweets。

```python
import keras
from keras.layers import Input, LSTM, Dense
from keras.models import Model

"""
每个tweets用280*256大小的矩阵表示，280是tweets的长度，256是每个字符的编码向量的纬度。
"""
tweet_a = Input(shape(280,256))
tweet_b = Input(shape(280,256))

"""
共享同一个编码层LSTM，这个LSTM将输出一个64维的向量。
"""
shared_lstm = LSTM(64)

encoded_a = shared_lstm(tweet_a)
encoded_b = shared_lstm(tweet_b)

merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)

predictions = Dense(1, activation='sigmoid')(merged_vector)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])
model.fit([data_a, data_b], labels, epochs=10)
```

## layer "node"概念
