[TOC]

# 核心层

## 1. Dense

```python
keras.layers.Dense(units, activtion=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
```

Dense表示的就是一个常规的全连接层。

它实现了如下的一个操作：output = activation(dot(input, kernel) + bias)。

注意：如果input的秩大于2，那么它会被展平，然后才和kernel进行点乘。

- units: 正整数，表示输出的纬度。
- activation: 激活函数，如果不指定，那么激活函数就是a(x) = x。
- use_bias: bool，是否使用偏差向量。
- kernel_initializer: 权重矩阵的初始化器。
- bias_initializer: 偏差的初始化器。
- kernel_regularizer: 权重矩阵的正则化器。
- bias_regularizer: 偏差的正则化器。
- activity_regularizer: 这一层输出的正则化器。
- kernel_constraint: 权重的约束函数。
- bias_constraint: 偏差的约束函数。

输入形状：(batch_size,...,input_dim)。

输出形状：(batch_size,...,units)。

## 2. Activation

作用就是在一个单元的输出上作用一个激活函数。

```python
keras.layers.Activation(activation)
```

- activation参数：激活函数的name或者是Tensorflow的一个operation。

## 3. Dropout

```python
keras.layers.Dropout(rate, noise_shape=None, seed=None)
```

Dropout是一种防止过拟合的方式，作用的原理就是rate比例的input units设置为0，相当于一个dropout的作用。

- rate：0~1之间的浮点数，代表要被drop的比例。
- noise_shape: 一维的整数张量，表示0-1 dropout掩码的形状，这个0-1掩码会与input相乘。通过这个可以控制对input的哪些纬度进行dropout。

## 4. Flatten

Flatten输入，但是不会影响batch size。

```python
keras.layers.Flatten(data_format=None)
```

- data_format: 数据的格式。有两种：channels_last和channels_first，默认是channels_last。channels_last对应的输入形状为(batch,...,channels)，channels_first对应的输入形状为(batch,channels,...)。默认是~/.keras/keras.json中的image_data_format配置项指定的格式，如果没有配置就是channels_last格式。

例子：

```python
model = Sequential()
model.add(Conv2D(64,(3,3), input_shape=(3,32,32), padding='same'))
# now model.output_shape = (None, 64, 32, 32)

model.add(Flatten())
# now shape is (None,65536)
```

## 5. Input

Input()用于实例化一个Keras tensor，所谓的keras tensor就是底层后端的tensor，同时增强了几个Keras自己的属性。

Keras增加了两个属性：_keras_shape，一个整数的shape tensor，通过关于shape推理来完成传播。_keras_history，应用于这个tensor的最后一层。整个图结构就是通过递归的访问那一层来获取的。

- shape: 不包含batch size的形状tensor。比如shape=(32,)，表示希望输入是一个32维的向量。
- batch_shape: 包含batch_size的shape tensor。
- name: 这一层的name。在一个model中，这个name应该是唯一的，如果不提供就会自动生成。
- dtype: 希望输入的数据类型。str类型，比如'float32'、'int32'等。
- sparse: bool，输入是否是稀疏数据。
- tensor: 一个要被包装为Input的tensor，如果设置了这个参数，那么就不会创建一个tensor的placeholder。

## 6. Reshape

```python
keras.layers.Reshape(target_shape)
```

- target_shape: 目标形状，不包含batch轴的信息。输出形状是:(batch_size,) + target_shape

## 7. Permute

对输入张量进行转置。

```python
keras.layers.Permute(dims)
```

- dims: 整数元组。转置的模式，不包含样本纬度。index从1开始。

```python
model = Sequential()
model.add(Permute((2,1), input_shape=(10,64)))
# now model.output_shape == (None, 64, 10)
```

## 8. RepeatVector

将输入数据重复n次。

```python
keras.layers.RepeatVector(n)
```

例子：

```python
model = Sequential()
model.add(Dense(32, input_dim=32))
# now model.output_shape == (None, 32)

model.add(RepeatVector(3))
# now model.output_shape == (None, 3, 32)
```

## 9. Lambda

作用就是封装一个表达式作为layer。

```python
keras.layers.Lambda(funtion, output_shape=None, mask=None, arguments=None)
```

例子：

```python
model.add(Lambda(lambda x: x ** 2))
```

```python
def antirectifier(x):
    x -= K.mean(x, axis=1, keepdims=True)
    x = K.l2_normalize(x, axis=1)
    pos = K.relu(x)
    neg = K.relu(-x)
    return K.concatenate([pos, neg], axis=1)

def antirectifier_output_shape(input_shape):
    shape = list(input_shape)
    shape[-1] *= 2
    return tuple(shape)

model.add(Lambda(antirectifier, output_shape=antirectifier_output_shape))
```

```python
# add a layer that returns the hadamard product
# and sum of it from two input tensors

def hadamard_product_sum(tensors):
    out1 = tensors[0] * tensors[1]
    out2 = K.sum(out1, axis=-1)
    return [out1, out2]

def hadamard_product_sum_output_shape(input_shapes):
    shape1 = list(input_shapes[0])
    shape2 = list(input_shapes[1])
    assert shape1 == shape2  # else hadamard product isn't possible
    return [tuple(shape1), tuple(shape2[:-1])]

x1 = Dense(32)(input_1)
x2 = Dense(32)(input_2)
layer = Lambda(hadamard_product_sum, hadamard_product_sum_output_shape)
x_hadamard, x_sum = layer([x1, x2])
```

## 10. ActivityRegularization

```python
keras.layers.ActivityRegularization(l1=0.0,l2=0.0)
```

正则项。

- l1: L1正则因子。
- l2: L2正则因子。

## 11. Masking

```python
keras.layers.Masking(mask_value=0.0)
```

## 12. SpatialDropout1D

```python
keras.layers.SpatialDropout1D(rate)
```

这个Layer类继承了Dropout类，重写了_get_noise_shape方法。它drop的不是某个元素，而是整个一维的特征。

If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.

## 13. SpatialDropout2D

```python
keras.layers.SpatialDropout2D(rate, data_format=None)
```

## 14. SpatialDropout3D

```python
keras.layers.SpatialDropout3D(rate, data_format=None)
```
