[TOC]

# Word embeddings

## 1. 文本数字化

机器学习算法都是把文本数组转换为数值化的向量进行处理，所以面对文本任务，要做的第一件事就是找到一个合适的方法进行“文本数字化”，将文本向量化。下面介绍三种文本向量化的策略。

### 1.1 one-hot encoding

向量长度等于词汇量大小，向量的每个位置表示一个固定的词。一个词的one-hot编码就是在这个词的位置上置1得到的向量。

![one-hot](./images/one-hot.png)

one-hot编码得到的词向量是高维稀疏向量。关于one-hot编码的更多内容，参考AIStudy -> 特征工程 -> one-hot&dummy编码。

### 1.2 每个单词用一个唯一的数字

比如一个词表中有100个词，那么可以分别用1,2,...,100来表示这些词。

这种表示有如下缺点：

1. 词的数字表示是任意的，不能反映词之间的关系，one-hot编码其实也有这个问题。
2. 模型无法解释。因为词的表示作为特征时，模型学习到的权重是没有明确意义的。因为模型学习到的是数字表示之间的关系，比如2和5的关系，但是2和5之间的关系和2，5背后真正词之间的关系是不对应的。

### 1.3 word embeddings

词嵌入的表示方式是一种稠密低维表示方式，这种表示不需要人为的指定，它是通过某种学习方式学得的。
